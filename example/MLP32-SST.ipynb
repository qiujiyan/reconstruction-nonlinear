{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../util/')\n",
    "sys.path.append('../datasets/')\n",
    "sys.path.append('../models/')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from scipy.optimize import fmin_cg\n",
    "import scipy.signal\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import *\n",
    "\n",
    "from engine import setup_seed,Namespace,train_vae_one_epoch_test\n",
    "from datasets import build_dataset\n",
    "from models import build_models\n",
    "from monitor import Monitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = Namespace()\n",
    "\n",
    "args.dataset_type ='cylinder2d-p'\n",
    "args.dataset_path =  r'../Sea_Surface_Temperature.npy'\n",
    "args.dataset_mask_path =  r'../Sea_Surface_Temperature_mask.npy'\n",
    "args.test_size = 0.33\n",
    "\n",
    "args.model_type = \"MLP\"\n",
    "args.mod_number = 32\n",
    "args.pod_loss = None\n",
    "\n",
    "args.lr = 1e-4\n",
    "args.step_size = 20\n",
    "args.gamma = 0.1\n",
    "args.epochs=int(50)\n",
    "\n",
    "args.log_interval = 100\n",
    "args.batch_size = 10\n",
    "args.device='cuda:0'\n",
    "args.word_dir='work_dir/'\n",
    "\n",
    "\n",
    "args.monitorType = \"random,sea\"\n",
    "# args.monitorGridShape = (5,5)\n",
    "args.random_num = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri_Mar_24_13_06_03_2023\n"
     ]
    }
   ],
   "source": [
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "localtime = localtime.replace(' ','_')\n",
    "localtime = localtime.replace(':','_')\n",
    "print(localtime)\n",
    "log_path = './log/'+localtime\n",
    "\n",
    "os.makedirs(log_path)\n",
    "\n",
    "\n",
    "    \n",
    "with open(log_path+'/arg.txt','w',encoding='utf-8') as f:\n",
    "    f.write(str(vars(args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_dataset.shape (180, 360)\n",
      "train_dataset.shape (180, 360)\n"
     ]
    }
   ],
   "source": [
    "build_dataset_res= build_dataset(args.dataset_path,args.dataset_type,args.test_size,)\n",
    "train_dataset = build_dataset_res['train_dataset']\n",
    "val_dataset  = build_dataset_res['val_dataset']\n",
    "\n",
    "TM =  train_dataset.data.mean(axis=0)\n",
    "train_dataset.data = train_dataset.data - train_dataset.data.mean(axis=0)\n",
    "\n",
    "args.data_shape =  build_dataset_res['data_shape']\n",
    "\n",
    "print(\"val_dataset.shape\",val_dataset.shape)\n",
    "print(\"train_dataset.shape\",train_dataset.shape)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=args.batch_size)\n",
    "val_loader = DataLoader(val_dataset,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model = build_models(\n",
    "    args.model_type,\n",
    "    args.data_shape,\n",
    "    args.mod_number,\n",
    "    args.pod_loss\n",
    ")\n",
    "model = build_model['model']\n",
    "args.mod_input_shape=build_model['mod_input_shape']\n",
    "args.mod_output_shape=build_model['mod_output_shape']\n",
    "args.code_shape=build_model['code_shape']\n",
    "\n",
    "\n",
    "with open(log_path+'/model.txt','w',encoding='utf-8') as f:\n",
    "    f.write(str(model))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,mode='min', factor=args.gamma, patience=args.step_size, threshold=0.00001,)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_one_epoch_test(\n",
    "        model: torch.nn.Module,\n",
    "        train_loader, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        device: torch.device, \n",
    "        epoch: int, \n",
    "        mod_input_shape,\n",
    "        mod_output_shape,\n",
    "        log_interval,\n",
    "        ):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        B_size = len(data)\n",
    "        B_mod_input_shape = [B_size,]+ list(mod_input_shape)\n",
    "        B_mod_output_shape = [B_size,]+ list(mod_output_shape)\n",
    "        output = model(data.reshape(B_mod_input_shape))\n",
    "        loss_info = model.loss_function(data.reshape(B_mod_output_shape),output)\n",
    "        loss = loss_info['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# val fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = val_dataset[15]-TM\n",
    "SSTmask = np.load(args.dataset_mask_path) \n",
    "\"\"\"\n",
    "Monitor(self,monitorType ,data_shape,\n",
    "        random_num=None, monitorGridShape=None\n",
    "\"\"\"\n",
    "meansure = Monitor(args.monitorType, args.data_shape,random_num = args.random_num ,mask = np.load(args.dataset_mask_path)  )\n",
    "mask_map = meansure.grid2D()\n",
    "mask_map = torch.Tensor(mask_map)\n",
    "\n",
    "    \n",
    "def val(targets : List ,m_s  : List ,model,code_shape,output_shape,device):\n",
    "    con_m=[]\n",
    "    for m in m_s:\n",
    "        res = []\n",
    "        for target in targets:\n",
    "            model = model.to(device)\n",
    "            mask_map = m.grid2D()\n",
    "            mask_map = torch.Tensor(mask_map)\n",
    "\n",
    "            target_re = target.reshape(output_shape).to(device)\n",
    "            mask_re   = mask_map.reshape(output_shape).to(device)\n",
    "\n",
    "            f = model.loss_decoder_helper(target_re,mask_re)\n",
    "            fp= model.grad_loss_decoder_helper(target_re,mask_re)\n",
    "\n",
    "            start_code = np.zeros(args.mod_number).astype(np.float32)+0.1\n",
    "            fmin_code =fmin_cg(f,start_code,fprime=fp,disp=False )\n",
    "            B_code_shape = [1,]+list(code_shape)\n",
    "            y_per = model.decode( torch.Tensor(fmin_code.astype(np.float32)).to(device).reshape(B_code_shape))\n",
    "\n",
    "            DS = (target.flatten().detach().cpu()-y_per.flatten().detach().cpu())\n",
    "            D = torch.sqrt((DS**2).sum())\n",
    "            T = torch.sqrt(((target.detach()+TM)**2).sum())\n",
    "            L2 = D/T\n",
    "            L2 = L2.cpu().numpy()\n",
    "            \n",
    "            res.append(L2)\n",
    "        res = np.array(res ).mean()\n",
    "        con_m.append(res)\n",
    "    return con_m \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1062 (0%)]\tLoss: 0.162871\n",
      "Train Epoch: 1 [1000/1062 (93%)]\tLoss: 0.136621\n",
      "Train Epoch: 2 [0/1062 (0%)]\tLoss: 0.161795\n",
      "Train Epoch: 2 [1000/1062 (93%)]\tLoss: 0.136848\n",
      "Train Epoch: 3 [0/1062 (0%)]\tLoss: 0.165830\n",
      "Train Epoch: 3 [1000/1062 (93%)]\tLoss: 0.137460\n",
      "Train Epoch: 4 [0/1062 (0%)]\tLoss: 0.162161\n",
      "Train Epoch: 4 [1000/1062 (93%)]\tLoss: 0.136596\n",
      "Train Epoch: 5 [0/1062 (0%)]\tLoss: 0.155549\n",
      "Train Epoch: 5 [1000/1062 (93%)]\tLoss: 0.143272\n",
      "Train Epoch: 6 [0/1062 (0%)]\tLoss: 0.159817\n",
      "Train Epoch: 6 [1000/1062 (93%)]\tLoss: 0.132921\n",
      "Train Epoch: 7 [0/1062 (0%)]\tLoss: 0.155876\n",
      "Train Epoch: 7 [1000/1062 (93%)]\tLoss: 0.136516\n",
      "Train Epoch: 8 [0/1062 (0%)]\tLoss: 0.161397\n",
      "Train Epoch: 8 [1000/1062 (93%)]\tLoss: 0.135818\n",
      "Train Epoch: 9 [0/1062 (0%)]\tLoss: 0.156751\n",
      "Train Epoch: 9 [1000/1062 (93%)]\tLoss: 0.141039\n",
      "Train Epoch: 10 [0/1062 (0%)]\tLoss: 0.158248\n",
      "Train Epoch: 10 [1000/1062 (93%)]\tLoss: 0.133863\n",
      "torch.save  ./log/MLP32-SST-best-0.475525.pth\n",
      "[0.4755251]\n",
      "Train Epoch: 11 [0/1062 (0%)]\tLoss: 0.157006\n",
      "Train Epoch: 11 [1000/1062 (93%)]\tLoss: 0.134666\n",
      "Train Epoch: 12 [0/1062 (0%)]\tLoss: 0.157698\n",
      "Train Epoch: 12 [1000/1062 (93%)]\tLoss: 0.134122\n",
      "Train Epoch: 13 [0/1062 (0%)]\tLoss: 0.160526\n",
      "Train Epoch: 13 [1000/1062 (93%)]\tLoss: 0.134402\n",
      "Train Epoch: 14 [0/1062 (0%)]\tLoss: 0.160638\n",
      "Train Epoch: 14 [1000/1062 (93%)]\tLoss: 0.135671\n",
      "Train Epoch: 15 [0/1062 (0%)]\tLoss: 0.159590\n",
      "Train Epoch: 15 [1000/1062 (93%)]\tLoss: 0.132889\n",
      "Train Epoch: 16 [0/1062 (0%)]\tLoss: 0.155265\n",
      "Train Epoch: 16 [1000/1062 (93%)]\tLoss: 0.126603\n",
      "Train Epoch: 17 [0/1062 (0%)]\tLoss: 0.154880\n",
      "Train Epoch: 17 [1000/1062 (93%)]\tLoss: 0.134096\n",
      "Train Epoch: 18 [0/1062 (0%)]\tLoss: 0.159777\n",
      "Train Epoch: 18 [1000/1062 (93%)]\tLoss: 0.131788\n",
      "Train Epoch: 19 [0/1062 (0%)]\tLoss: 0.161345\n",
      "Train Epoch: 19 [1000/1062 (93%)]\tLoss: 0.127055\n",
      "Train Epoch: 20 [0/1062 (0%)]\tLoss: 0.161741\n",
      "Train Epoch: 20 [1000/1062 (93%)]\tLoss: 0.129958\n",
      "[0.47817203]\n",
      "Train Epoch: 21 [0/1062 (0%)]\tLoss: 0.157776\n",
      "Train Epoch: 21 [1000/1062 (93%)]\tLoss: 0.127144\n",
      "Train Epoch: 22 [0/1062 (0%)]\tLoss: 0.160357\n",
      "Train Epoch: 22 [1000/1062 (93%)]\tLoss: 0.126966\n",
      "Train Epoch: 23 [0/1062 (0%)]\tLoss: 0.155357\n",
      "Train Epoch: 23 [1000/1062 (93%)]\tLoss: 0.125626\n",
      "Train Epoch: 24 [0/1062 (0%)]\tLoss: 0.151619\n",
      "Train Epoch: 24 [1000/1062 (93%)]\tLoss: 0.125118\n",
      "Train Epoch: 25 [0/1062 (0%)]\tLoss: 0.154518\n",
      "Train Epoch: 25 [1000/1062 (93%)]\tLoss: 0.124955\n",
      "Train Epoch: 26 [0/1062 (0%)]\tLoss: 0.152278\n",
      "Train Epoch: 26 [1000/1062 (93%)]\tLoss: 0.128953\n",
      "Train Epoch: 27 [0/1062 (0%)]\tLoss: 0.156956\n",
      "Train Epoch: 27 [1000/1062 (93%)]\tLoss: 0.125590\n",
      "Train Epoch: 28 [0/1062 (0%)]\tLoss: 0.152415\n",
      "Train Epoch: 28 [1000/1062 (93%)]\tLoss: 0.129234\n",
      "Train Epoch: 29 [0/1062 (0%)]\tLoss: 0.151470\n",
      "Train Epoch: 29 [1000/1062 (93%)]\tLoss: 0.125115\n",
      "Train Epoch: 30 [0/1062 (0%)]\tLoss: 0.150709\n",
      "Train Epoch: 30 [1000/1062 (93%)]\tLoss: 0.128353\n",
      "torch.save  ./log/MLP32-SST-best-0.472067.pth\n",
      "[0.4720672]\n",
      "Train Epoch: 31 [0/1062 (0%)]\tLoss: 0.151419\n",
      "Train Epoch: 31 [1000/1062 (93%)]\tLoss: 0.125019\n",
      "Train Epoch: 32 [0/1062 (0%)]\tLoss: 0.152939\n",
      "Train Epoch: 32 [1000/1062 (93%)]\tLoss: 0.124300\n",
      "Train Epoch: 33 [0/1062 (0%)]\tLoss: 0.151035\n",
      "Train Epoch: 33 [1000/1062 (93%)]\tLoss: 0.129949\n",
      "Train Epoch: 34 [0/1062 (0%)]\tLoss: 0.152070\n",
      "Train Epoch: 34 [1000/1062 (93%)]\tLoss: 0.127818\n",
      "Train Epoch: 35 [0/1062 (0%)]\tLoss: 0.148665\n",
      "Train Epoch: 35 [1000/1062 (93%)]\tLoss: 0.124522\n",
      "Train Epoch: 36 [0/1062 (0%)]\tLoss: 0.148471\n",
      "Train Epoch: 36 [1000/1062 (93%)]\tLoss: 0.125047\n",
      "Train Epoch: 37 [0/1062 (0%)]\tLoss: 0.155434\n",
      "Train Epoch: 37 [1000/1062 (93%)]\tLoss: 0.125664\n",
      "Train Epoch: 38 [0/1062 (0%)]\tLoss: 0.153671\n",
      "Train Epoch: 38 [1000/1062 (93%)]\tLoss: 0.127402\n",
      "Train Epoch: 39 [0/1062 (0%)]\tLoss: 0.147614\n",
      "Train Epoch: 39 [1000/1062 (93%)]\tLoss: 0.124326\n",
      "Train Epoch: 40 [0/1062 (0%)]\tLoss: 0.147814\n",
      "Train Epoch: 40 [1000/1062 (93%)]\tLoss: 0.122885\n",
      "[0.47667667]\n",
      "Train Epoch: 41 [0/1062 (0%)]\tLoss: 0.149263\n",
      "Train Epoch: 41 [1000/1062 (93%)]\tLoss: 0.123073\n",
      "Train Epoch: 42 [0/1062 (0%)]\tLoss: 0.154917\n",
      "Train Epoch: 42 [1000/1062 (93%)]\tLoss: 0.125202\n",
      "Train Epoch: 43 [0/1062 (0%)]\tLoss: 0.150122\n",
      "Train Epoch: 43 [1000/1062 (93%)]\tLoss: 0.126074\n",
      "Train Epoch: 44 [0/1062 (0%)]\tLoss: 0.149262\n",
      "Train Epoch: 44 [1000/1062 (93%)]\tLoss: 0.123438\n",
      "Train Epoch: 45 [0/1062 (0%)]\tLoss: 0.146525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\giteeProject\\reconstruction-nonlinear\\example\\MLP32-SST.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, args\u001b[39m.\u001b[39mepochs\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_vae_one_epoch_test( \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         model,  \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         train_loader, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         optimizer, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         args\u001b[39m.\u001b[39;49mdevice, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         epoch ,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         args\u001b[39m.\u001b[39;49mmod_input_shape,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         args\u001b[39m.\u001b[39;49mmod_output_shape,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         args\u001b[39m.\u001b[39;49mlog_interval\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m epoch\u001b[39m>\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         val_res \u001b[39m=\u001b[39m val([target,],[meansure,],model,args\u001b[39m.\u001b[39mcode_shape,args\u001b[39m.\u001b[39mmod_output_shape,args\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[1;32md:\\giteeProject\\reconstruction-nonlinear\\example\\MLP32-SST.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain_vae_one_epoch_test\u001b[1;34m(model, train_loader, optimizer, device, epoch, mod_input_shape, mod_output_shape, log_interval)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_vae_one_epoch_test\u001b[39m(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         model: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         train_loader, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         log_interval,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         ):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\giteeProject\\reconstruction-nonlinear\\example\\../datasets\\datasets.py:22\u001b[0m, in \u001b[0;36mFluidFieldDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m,i):\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[i],dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(args.device)\n",
    "\n",
    "val_best = np.inf\n",
    "\n",
    "for epoch in range(1, args.epochs+ 1):\n",
    "    model = model.to(args.device)\n",
    "    train_vae_one_epoch_test( \n",
    "        model,  \n",
    "        train_loader, \n",
    "        optimizer, \n",
    "        args.device, \n",
    "        epoch ,\n",
    "        args.mod_input_shape,\n",
    "        args.mod_output_shape,\n",
    "        args.log_interval\n",
    "    )\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0 and epoch>1:\n",
    "        val_res = val([target,],[meansure,],model,args.code_shape,args.mod_output_shape,args.device)\n",
    "        #def val(targets,m_s,model,code_shape,output_shape,device):\n",
    "        if val_best > val_res[0]:\n",
    "            val_best = val_res[0]\n",
    "            torch.save(model,log_path+'/MLP32-SST-best.pth')\n",
    "            torch.save(model,'./log/MLP32-SST-best-%f.pth'%(val_best))\n",
    "            print('torch.save ','./log/MLP32-SST-best-%f.pth'%(val_best))\n",
    "            \n",
    "        print(val_res)\n",
    "\n",
    "torch.save(model,log_path+'/last.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(target  ,m  ,model,code_shape,output_shape,device):\n",
    "\n",
    "    model = model.to(device)\n",
    "    mask_map,meansure_x,meansure_y = m.grid2D(reqxy=True)\n",
    "    mask_map = torch.Tensor(mask_map)\n",
    "\n",
    "    target_re = (target.float()).reshape(output_shape).to(device)\n",
    "    mask_re   = mask_map.reshape(output_shape).to(device)\n",
    "\n",
    "    f = model.loss_decoder_helper(target_re,mask_re)\n",
    "    fp= model.grad_loss_decoder_helper(target_re,mask_re)\n",
    "\n",
    "    start_code = np.zeros(args.mod_number).astype(np.float32)+0.1\n",
    "    fmin_code =fmin_cg(f,start_code,fprime=fp,disp=False )\n",
    "    B_code_shape = [1,]+list(code_shape)\n",
    "    y_per = model.decode( torch.Tensor(fmin_code.astype(np.float32)).to(device).reshape(B_code_shape))\n",
    "\n",
    "\n",
    "    DS = (target.flatten().detach().cpu()-y_per.flatten().detach().cpu())\n",
    "    D = torch.sqrt((DS**2).sum())\n",
    "    T = torch.sqrt(((target.detach()+TM)**2).sum())\n",
    "    L2 = D/T\n",
    "    L2 = L2.cpu().numpy()\n",
    "\n",
    "    y_per = y_per.flatten().detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    return y_per,L2,meansure_x,meansure_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = val_dataset[0]\n",
    "\n",
    "mm = Monitor(args.monitorType, args.data_shape,random_num = 20  ,mask = np.load(args.dataset_mask_path))\n",
    "val_res,L2,meansure_x,meansure_y = test(target - TM ,mm,model,args.code_shape,args.mod_output_shape,args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.03808995, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "\n",
    "def plot_save(x,name,xy=None):\n",
    "    x = ma.masked_array((x).reshape(args.data_shape),SSTmask)\n",
    "\n",
    "    plt.figure(figsize=(8,6),dpi=300)\n",
    "    plt.imshow(x,cmap='coolwarm',vmin=0, vmax=32)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    if xy is not None:\n",
    "        x,y = xy\n",
    "        plt.scatter(y,x,4,c='g')\n",
    "    plt.title(name)\n",
    "    plt.savefig(name+ '.png')\n",
    "    plt.savefig(name+ '.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '3' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\giteeProject\\reconstruction-nonlinear\\example\\MLP32-SST.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m signal \u001b[39mas\u001b[39;00m signal\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m kernel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mones(\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m target_c \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mconvolve2d(target,kernel)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/giteeProject/reconstruction-nonlinear/example/MLP32-SST.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plot_save(target_c,\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m,xy\u001b[39m=\u001b[39mmm\u001b[39m.\u001b[39mxy)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\numeric.py:204\u001b[0m, in \u001b[0;36mones\u001b[1;34m(shape, dtype, order, like)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m like \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     \u001b[39mreturn\u001b[39;00m _ones_with_like(shape, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder, like\u001b[39m=\u001b[39mlike)\n\u001b[1;32m--> 204\u001b[0m a \u001b[39m=\u001b[39m empty(shape, dtype, order)\n\u001b[0;32m    205\u001b[0m multiarray\u001b[39m.\u001b[39mcopyto(a, \u001b[39m1\u001b[39m, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '3' as a data type"
     ]
    }
   ],
   "source": [
    "from scipy import signal as signal\n",
    "\n",
    "kernel = np.ones(3,3)\n",
    "\n",
    "target_c = signal.convolve2d(target,kernel)\n",
    "\n",
    "\n",
    "plot_save(target_c,'target',xy=mm.xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_save(val_res+TM.flatten(),'$N_{sensors} = 20$',xy=mm.xy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baselne : GPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
